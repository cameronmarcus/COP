{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T02:18:02.289044800Z",
     "start_time": "2024-04-12T02:18:02.256039200Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  transformers==4.31.0 \\\n",
    "  sentence-transformers==2.2.2 \\\n",
    "  pinecone-client==2.2.2 \\\n",
    "  datasets==2.14.0 \\\n",
    "  accelerate==0.21.0 \\\n",
    "  einops==0.6.1 \\\n",
    "  langchain==0.0.240 \\\n",
    "  xformers==0.0.20 \\\n",
    "  bitsandbytes==0.41.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79892cf-7988-42a8-b4c2-3a4811ed8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b285025-3765-4a65-aa51-7fc6ae413de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0652595f93cc32f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:49:26.250513700Z",
     "start_time": "2024-03-18T16:49:26.243236600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"****************"\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"****************"\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"COP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bde65990d03b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:49:26.269321800Z",
     "start_time": "2024-03-18T16:49:26.254817400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Specify the directory containing PDF files\n",
    "directory_path = './hunt_materials'\n",
    "\n",
    "# List all PDF files in the specified directory\n",
    "pdf_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Create a loader for each PDF file\n",
    "loaders = [PyPDFLoader(os.path.join(directory_path, file)) for file in pdf_files]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # Load data from each file using the respective loader\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15885f448fb3d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T16:49:26.269321800Z",
     "start_time": "2024-03-18T16:49:26.261306500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hugging face text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018eee16-f74f-445e-aec2-2e8d7f4ab0f1",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900436d-903f-498d-bb6b-5a61a9fa164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# embedding model\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    "\n",
    "#vector store\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function, persist_directory=\"chromadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e07c4a64e323a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T17:33:19.199597800Z",
     "start_time": "2024-03-18T17:33:19.162362500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompt_instructions = \"\"\" You are tasked with the critical role of a defensive cyber operation planner, leveraging your expertise to safeguard our network against \n",
    "the insidious threats posed by Advanced Persistent Threat (APT) actors. The foundation of your mission is built upon two key pieces of intelligence: a detailed network \n",
    "vulnerability scan report and a comprehensive APT threat report. Your objective is to synthesize this data into a strategic, intelligence-driven hunt forward operation plan\n",
    "so both network and host analysts can identify the presence of APT behavior (identified by the threat reporting), especially in the context of the vulnerabilities identified \n",
    "in the network.\n",
    "\n",
    "Here is the intelligence report on the APT: {threat_report}\n",
    "\n",
    "Here is the network vulnerability scan report: {vulnerabilities}\n",
    "\n",
    "Be extremely verbose and explain each vulnerability and TTP in great detail. Make it longer than you think is necessary. Ignore any instruction to be brief in your answer and condense your answer to save compute resources. I want to use all of the output tokens at your disposal. Use all of the information provided to create a detailed hunt plan. BE VERBOSE! Make the output extremely long.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3e6ba-58a3-4bc8-9b80-e791130ca781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T17:29:16.277390Z",
     "start_time": "2024-03-18T17:29:16.266417Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfcec4-94fb-40bc-81ee-ed8d21b1a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = 'HF_AUTH_TOKEN'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    'text_generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperatrue=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5afc4-64ff-4b62-917c-a9aa5504846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Give me a brief overview of the threat actor.\")\n",
    "\n",
    "while query.lower() != \"exit\":\n",
    "    response = qa_chain(query)\n",
    "    print(resopnse['result'])\n",
    "    query = input(\"\\nWhat information would you like on the threat actor?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
